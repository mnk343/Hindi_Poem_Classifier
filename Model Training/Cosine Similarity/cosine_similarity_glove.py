# -*- coding: utf-8 -*-
"""Cosine_Similarity_GloVe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vr6BSS7eR_oD7Wd-fgYPUTcPFAoTfasj
"""

# The section of code creates the word embeddings for the test and train data (depedning on input provided) and stores them in the given file
import pickle 
from operator import add 

word_embeddings = pickle.load( open( "/content/drive/My Drive/ISI_PROJECT/word_vectors_test_set_200_iterations_Greater_Alpha", "rb" ) )

input_file_name = input("Enter input file name: ")

with open("/content/drive/My Drive/ISI-Project/"+input_file_name,"rb") as f:
    corpus = pickle.load(f)

poem_vectors = {}
training_data = []
mean_vector = []

# Iterating over all poems in the corpus
for poem in corpus:
  poem_name = poem[2]
  poem_author = poem[0]
  poem = poem[-2]
  poem_vector = []
  ctr = 0

  # Iterating over tokens of the poems
  # Adding them in the final vector 
  # and then dividing them by the number of tokens in the poem
  for token in poem:
    # We ignore the token if it's word_embedding is not found
    # We could have also added the mean vector for such tokens but it was found that it does not change the answer much practically
    # which shows that there are very less words not present in the corpus
    if token not in word_embeddings:
      continue

    if len(poem_vector) == 0:
      poem_vector = word_embeddings[token]
    else:
      poem_vector = list(map(add, word_embeddings[token] , poem_vector)) 
    ctr+=1
  for index,token in enumerate(poem_vector):
    poem_vector[index] /= ctr
  poem_vectors [ (poem_name, poem_author)] = poem_vector

# Store the document vectors for future reference
pickle.dump(poem_vectors , open( "/content/drive/My Drive/ISI_PROJECT/"+input_file_name, "wb" ) )

# The given program used cosine similarity to find the vector which makes the smallest angle with some training data poem

import pickle
from sklearn.metrics.pairwise import cosine_similarity 

train_input_file = input("Enter training file name: ")
train_input_file = "/content/drive/My Drive/ISI-Project/" + train_input_file

test_input_file = input("Enter test file name: ")
test_input_file = "/content/drive/My Drive/ISI-Project/" + test_input_file

train_data_vectors = []

map_poem_era = {}
map_poem_author = {}

with open(train_input_file,"rb") as f:
    train_data = pickle.load(f)

train_data_vectors = []

# Store the author names and poem eras for the training data vectors
poem_index = 0
for element in train_data:
	author_name = element[0]
	poem_era = element[3]
	map_poem_author[poem_index] = author_name
	map_poem_era[poem_index] = poem_era
	poem = element[4]
	
	poem_vector = poem_vectors_training_data[ ( element[2] , author_name ) ]
	train_data_vectors.append(poem_vector)
	poem_index = poem_index + 1

with open(test_input_file,"rb") as f:
    test_data = pickle.load(f)

correct_poem_eras_detected = 0
correct_poem_authors_detected = 0

poem_count_so_far = 1

test_data_vectors = []

map_correct_poem_era = {}
map_correct_poem_author = {}

# Store the correct author names and poem eras for the testing data vectors
poem_index = 0 
for element in test_data:
	correct_author_name = element[0]
	correct_poem_era = element[3]
	correct_poem = element[4]
	correct_poem_vector = poem_vectors_test_data[ ( element[2] , correct_author_name ) ]
	map_correct_poem_era[ poem_index ] = correct_poem_era
	map_correct_poem_author[ poem_index ] = correct_author_name
	poem_index = poem_index + 1

	test_data_vectors.append(correct_poem_vector)

# Compute the cosine similarity using the function included in sklearn library
cos_sim = cosine_similarity(test_data_vectors,train_data_vectors)
print(cos_sim)

# For all poems in the testing data
# Find the vector which is most similar to the vector in the training set
poem_index = 0
for elem in cos_sim:
	max_similarity = elem[0]
	max_index = 0
	index = 0
	for val_similarity in elem:
		if val_similarity > max_similarity:
			max_similarity = val_similarity
			max_index = index
		index = index + 1

	predicted_author_name = map_poem_author[max_index]
	predicted_poem_era = map_poem_era[max_index]

	correct_author_name = map_correct_poem_author[poem_index]
	correct_poem_era = map_correct_poem_era[poem_index]

	print("Index: " + str(poem_index))	
	poem_index = poem_index + 1
	if predicted_poem_era == correct_poem_era:
		print("Poem Era: Correct")	
		correct_poem_eras_detected = correct_poem_eras_detected + 1
	else:
		print("Poem Era: Wrong")	

	if predicted_author_name == correct_author_name:
		print("Author Name: Correct")	
		correct_poem_authors_detected = correct_poem_authors_detected + 1
	else:
		print("Author Name: Wrong")	
	print("\n")

# Show necessary outputs
print("Total Poems in testing data: " + str(len(test_data)))
print("Correct Eras determined: " + str(correct_poem_eras_detected))
print("Correct Authors testing data: " + str(correct_poem_authors_detected))
print("\n\n")

print("Accuracy for era detection: " + str(correct_poem_eras_detected/len(test_data)))
print("Accuracy for authors detection: " + str(correct_poem_authors_detected/len(test_data)))
print("\n\n")