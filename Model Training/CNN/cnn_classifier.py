# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k284pJPhTXqXBUrPhfm_eJov-dxX52LO
"""

!pip install tensorflow
!pip install keras

from google.colab import drive
drive.mount("/content/drive")

import pickle
with open('/content/drive/MyDrive/pro/test_data_bow', 'rb') as tf:
    test_bow = pickle.load(tf)

with open('/content/drive/MyDrive/pro/train_data_bow', 'rb') as f:
    bow = pickle.load(f)
print(bow[0])

poets = []
eras = []
vectors = []
count = 0
temp = {}
for row in bow:
  poets.append(row[0])
  if row[3] not in temp:
    count += 1
    temp[row[3]] = count
  eras.append(temp[row[3]])
  vectors.append(row[5])

test_poets = []
test_eras = []
test_vectors = []
for row in test_bow:
  test_poets.append(row[3])
  if (row[3] in temp.keys()):
    test_eras.append(temp[row[3]])
  else:
    test_eras.append(-1)
  test_vectors.append(row[5])

print(temp)

from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Convolution2D
from keras.layers import Dropout
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence

from sklearn.model_selection import train_test_split
import numpy as np

# Our dictionary will contain only of the top 7000 words appearing most frequently
top_words = 7000
# Now we split our data-set into training and test data
### (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)
# Looking at the nature of training data
# X_train, X_test, y_train, y_test = train_test_split(vectors, eras, test_size=0.1, random_state=42)
X_train = np.asarray(vectors)
X_test = np.asarray(test_vectors)
y_train = np.asarray(eras).reshape(-1,1)
y_test = np.asarray(test_eras).reshape(-1,1)
print(X_train[0])
print(y_train[0])
print('Shape of training data: ')
print(X_train.shape)
print(y_train.shape)
print('Shape of test data: ')
print(X_test.shape)
print(y_test.shape)

# Padding the data samples to a maximum poem length in words
max_words = 450
X_train = sequence.pad_sequences(X_train, maxlen=max_words)
X_test = sequence.pad_sequences(X_test, maxlen=max_words)
# Building the CNN Model
model = Sequential()      # initializing the Sequential nature for CNN model

# Adding the embedding layer
model.add(Embedding(top_words, 32, input_length=max_words))
# model.add(Conv1D(32, 3, padding='same', activation='relu'))
# model.add(MaxPooling1D())
# model.add(Flatten())
# model.add(Dense(250, activation='relu'))
# model.add(Dense(4, activation='softmax'))

model.add(Conv1D(32, 3, padding='same', activation='relu'))
# model.add(Convolution2D(32,3,3,input_shape=(32,32,3), activation='relu'))
model.add(Dropout(.1))
model.add(MaxPooling1D())
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(16, activation='softmax'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# model.summary()

# Fitting the data onto model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128, verbose=2)
# Getting score metrics from our model
scores = model.evaluate(X_test, y_test, verbose=0)
# Displays the accuracy of correct sentiment prediction over test data
print("Accuracy: %.2f%%" % (scores[1]*100))